<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Chihye Han (Kelsey) | Johns Hopkins University </title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <center>
        <img src="images/profile.jpg" alt="Profile" WIDTH=190 style="margin-bottom: 8px;border-radius: 10px;">
        <h1>Kelsey Han</h1>
        <p>PhD Student</br>
        Department of Cognitive Science</br>
        Johns Hopkins University</br>
        <a href="mailto:chan21@jhu.edu">chan21@jhu.edu</a></p>
	<a href="han_cv.pdf" style="margin-right:14px;"><i class="ai ai-cv" style="font-size: 32px;"></i>
        </a>
        <a href="https://github.com/kelseyhan-jhu" style="margin-right:14px;">
          <i class="fa fa-github" style="font-size:32px;"></i>
        </a>
        <a href="https://scholar.google.com/citations?hl=en&user=ZN77OiqrzrIC" onclick="_gaq.push(['_trackEvent', 'Click', 'Scholar Clicked']);" style="margin-right:14px;">
          <i class="ai ai-google-scholar big-icon" style="font-size:32px;"></i>
        </a>
        <a href="https://twitter.com/ckelseyhan" onclick="_gaq.push(['_trackEvent', 'Click', 'Twitter Clicked']);">
          <i class="fa fa-twitter" style="font-size:32px;"></i>
        </a>
        </center>
        <!--p class="view"><a href="http://github.com/orderedlist/minimal">View the Project on GitHub <small>orderedlist/minimal</small></a></p-->
        <!--ul>
          <li><a href="https://github.com/orderedlist/minimal/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="http://github.com/orderedlist/minimal">Fork On <strong>GitHub</strong></a></li>
        </ul-->
      </header>
      <section>
        <p>Welcome! I am a Ph.D. student advised by Prof. Michael Bonner at <a href="http://cogsci.jhu.edu">Johns Hopkins University Department of Cognitive Science.</a> </p>
        <p>My research interests involve leveraging computational modeling and neuroimaging techniques for understanding representations in biological and artificial systems.</p>
        <p>Prior to my doctoral training, I received M.S. in Electrical Engineering from KAIST and B.A. in Cognitive Science (Minor: Neuroscience) from Carleton College. I worked as a software engineer at Software R&#38;D Center, Samsung Electronics and as a data analyst at a <a href="http://obelab.com">brain imaging device manufacturing startup</a> incubated at KAIST.
</p>

        <h3>News</h3>
        <ul>
          <li>Mar 2021: First-year project at JHU CogSci is accepted for a poster presentation at VSS 2021, with <a href="https://myemail.constantcontact.com/VSS-2021---Recipients-of-the-2021-Travel-Awards.html?soid=1123842305942&aid=6JERfVofEEE">Elseveir/<i>Vision Research </i>Travel Award</a>. 
          <li>Sep 2020: I joined <a href="http://https://bonnerlab.org/">BonnerLab</a> as a PhD student!
        </ul>


        <h3>Research</h3>
        <table style="margin-top:-12px;">
		<tr style="border-bottom:1px solid #e5e5e5;">
		  <td>
                    <img src="images/semdim.png" alt="Semantic dimensionality">
                  </td>
                  <td>
                    <b>Quantifying the latent semantic content of visual representations</b> (VSS 2021)
                    <br><br>
                    <p>How does visual cortex extract semantic meaning from images? To address this question, we developed a statistical measure called semantic dimensionality that quantifies the number of language-derived semantic properties that can be decoded from a set of image-computable perceptual features.</p>
                    <br>
                    <p>By me, Caterina Magri, Michael Bonner</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/mhsan.png" alt="MHSAN">
                  </td>
                  <td>
                    <b><a href="https://ieeexplore.ieee.org/document/9093548">MHSAN: Multi-head self-attention network for visual semantic embedding</a></b> (WACV 2020)
                    <br><br>
                    <p>We propose a novel multi-head self-attention network to attend to  various components of visual and textual data. Our approach achieves an effective and interpretable visual-semantic joint space and obtains new state-of-the-art results in image-text retrieval tasks.</p>
                    <br>
                    <p>By Geondo Park, me, Wonjoon Yoon, Daeshik Kim</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/adversarialfMRI.png" alt="Adversarial fMRI">
                  </td>
                  <td>
                    <b><a href="https://ieeexplore.ieee.org/document/8851763">Representation of adversarial examples in humans and deep neural networks: an fMRI study</a></b> (IJCNN 2019; WiML 2019)
                    <br><br>
                    <p>Deep neural networks are vulnerable to adversarial examples, input images on which subtle, carefully designed noises are added to fool a machine classifier. We compare the visual representations of adversarial examples in deep neural networks and humans with functional magnetic resonance imaging (fMRI) and the representational similarity analysis (RSA) framework.</p>
                    <br>
                    <p>By me, Wonjun Yoon, Gihyun Kwon, Seungkyu Nam, Daeshik Kim</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/3dbrain.png" alt="3D brain">
                  </td>
                  <td>
                    <b><a href="https://arxiv.org/abs/1908.02498">Generation of 3D brain MRI using auto-encoding generative adversarial networks</a></b> (MICCAI 2019)
                    <br><br>
                    <p>We generate realistic brain MR images of multiple types (e.g. normal or diseased) and modalities (e.g. T2 or FLAIR) from scratch, as opposed to image-to-image translation, by leveraging alpha-GAN with WGAN-GP.</p>
                    <br>
                    <p>By Gihyun Kwon, me, Daeshik Kim</p>
                  </td>
                </tr>

        </table>
      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
