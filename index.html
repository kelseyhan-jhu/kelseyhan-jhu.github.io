<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kelsey Han | Johns Hopkins University </title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <center>
        <img src="images/profile.jpg" alt="Profile" WIDTH=190 style="margin-bottom: 8px;border-radius: 10px;">
        <h1>Kelsey Han</h1>
        <p style="font-size: 16px; margin-top: -8px; margin-bottom: 8px;">Chihye 지혜 [d͡ʑi.e]</p>
        <p>Department of Cognitive Science</br>
        Johns Hopkins University</br>
        <a href="mailto:chan21@jhu.edu">chan21@jhu.edu</a></p>
	<a href="han_cv.pdf" style="margin-right:14px;"><i class="ai ai-cv" style="font-size: 32px;"></i>
        </a>
        <a href="https://github.com/kelseyhan-jhu" style="margin-right:14px;">
          <i class="fa fa-github" style="font-size:32px;"></i>
        </a>
        <a href="https://scholar.google.com/citations?hl=en&user=ZN77OiqrzrIC" onclick="_gaq.push(['_trackEvent', 'Click', 'Scholar Clicked']);" style="margin-right:14px;">
          <i class="ai ai-google-scholar big-icon" style="font-size:32px;"></i>
        </a>
        <a href="https://twitter.com/ckelseyhan" onclick="_gaq.push(['_trackEvent', 'Click', 'Twitter Clicked']);">
          <i class="fa fa-twitter" style="font-size:32px;"></i>
        </a>
        <br>
        </center>
      </header>
      <section>
        <p>I am a Ph.D. candidate in Cognitive Science at Johns Hopkins University, advised by Prof. Michael F. Bonner. I study the neural basis of visual experience using computational modeling, neuroimaging, and deep learning approaches. My research focuses on understanding how high-dimensional neural geometries give rise to complex, rich visual perception.</p>

        <p>Previously, I received my M.S. in Electrical Engineering from KAIST and B.A. in Cognitive Science from Carleton College. I have also worked as an AI research strategist at LG AI Research and as a software engineer at Samsung Electronics.</p>

        <h3>Updates</h3>
        <ul>
          <li>Aug 2025: Presented at CCN 2025 in Amsterdam, Netherlands.</li>
          <li>Mar 2025: <a href="https://arxiv.org/abs/2505.12653">New preprint</a> on high-dimensional structure underlying individual differences in naturalistic visual experience.</li>
          <li>Aug 2024: Presented at CCN 2024 in Boston, MA.</li>
          <li>May 2024: Presented at VSS 2024 in St. Petersburg, FL.</li>
        </ul>

        <h3>Research</h3>
        <table style="margin-top:-12px;">
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/ccn2025.png" alt="Behavioral relevance">
                  </td>
                  <td>
                    <b><a href="https://2025.ccneuro.org/abstract_pdf/Han_2025_Behavioral_relevance_high-dimensional_neural_representations.pdf">Behavioral relevance of high-dimensional neural representations</a></b>
                    <br><i>Cognitive Computational Neuroscience, 2025</i>
                    <br><br>
                    <p>Does behavioral relevance extend throughout high-dimensional neural representations or is it restricted to interpretable, high-variance dimensions? We show that humans can perceive coherent structure in image clusters formed along principal components spanning the entire spectrum of ventral visual stream responses, suggesting that behaviorally relevant information is distributed across the full range of neural dimensions.</p>
                    <p>Han, C., Gauthaman, R.M., & Bonner, M.F.</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/indiv.png" alt="Individual difference">
                  </td>
                  <td>
                    <b><a href="https://arxiv.org/abs/2505.12653">High-dimensional structure underlying individual differences in naturalistic visual experience</a></b>
                    <br><br>
                    <p>How do different brains create unique visual experiences from identical sensory input? We reveal that individual visual experience emerges from a high-dimensional neural geometry across the visual cortical hierarchy.</p>
                    <p>Han, C. & Bonner, M.F.</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/quantsem.png" alt="Semantic dimensionality">
                  </td>
                  <td>
                    <b><a href="VSS_Han_presentation.pdf">Quantifying the latent semantic content of visual representations</a></b>
                    <br><i>Vision Sciences Society, 2021</i>
                    <br><br>
                    <p>How does visual cortex extract semantic meaning from images? We developed a statistical measure called semantic dimensionality that quantifies the number of language-derived semantic properties that can be decoded from image-computable perceptual features.</p>
                    <p>Han, C., Magri, C., & Bonner, M.F.</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/mhsan.png" alt="MHSAN">
                  </td>
                  <td>
                    <b><a href="https://ieeexplore.ieee.org/document/9093548">MHSAN: Multi-head self-attention network for visual semantic embedding</a></b>
                    <br><i>WACV, 2020</i>
                    <br><br>
                    <p>We propose a multi-head self-attention network to attend to various components of visual and textual data, achieving state-of-the-art results in image-text retrieval tasks.</p>
                    <p>Park, G., Han, C., Yoon, W., & Kim, D.</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/adversarialfMRI.png" alt="Adversarial fMRI">
                  </td>
                  <td>
                    <b><a href="https://ieeexplore.ieee.org/document/8851763">Representation of adversarial examples in humans and deep neural networks: an fMRI study</a></b>
                    <br><i>IJCNN, 2019</i>
                    <br><br>
                    <p>We compare the visual representations of adversarial examples in deep neural networks and humans using fMRI and representational similarity analysis.</p>
                    <p>Han, C., Yoon, W., Kwon, G., Nam, S., & Kim, D.</p>
                  </td>
                </tr>
                <tr style="border-bottom:1px solid #e5e5e5;">
                  <td>
                    <img src="images/3dbrain.png" alt="3D brain">
                  </td>
                  <td>
                    <b><a href="https://arxiv.org/abs/1908.02498">Generation of 3D brain MRI using auto-encoding generative adversarial networks</a></b>
                    <br><i>MICCAI, 2019</i>
                    <br><br>
                    <p>We generate realistic brain MR images of multiple types and modalities from scratch using alpha-GAN with WGAN-GP.</p>
                    <p>Kwon, G., Han, C., & Kim, D.</p>
                  </td>
                </tr>
        </table>
      </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
